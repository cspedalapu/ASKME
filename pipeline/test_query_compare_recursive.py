#Supports three modes: vector-only, rerank, and full GPT answer generation

import chromadb
from sentence_transformers import SentenceTransformer, CrossEncoder
from transformers import AutoTokenizer
import torch
import argparse
import openai
import os
import json

# CONFIGURATION
EMBEDDING_MODEL = "BAAI/bge-m3"
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
CHROMA_DB_PATH = "../output/chroma_store_recursive"
COLLECTION_NAME = "document_chunks_recursive"
TOP_K = 10
TOP_RERANKED = 5
USE_OPENAI = True


# SETUP
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

embedder = SentenceTransformer(EMBEDDING_MODEL).to(device)
tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)
reranker = CrossEncoder(RERANKER_MODEL, device=device)

chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = chroma_client.get_collection(name=COLLECTION_NAME)


# HELPERS
def is_text_natural(text):
    alpha_ratio = sum(c.isalpha() for c in text) / max(len(text), 1)
    return alpha_ratio > 0.3 and '{' not in text and '}' not in text

def generate_answer(chunks, question):
    context = "\n\n".join([f"[{i+1}] {chunk}" for i, chunk in enumerate(chunks)])
    prompt = f"""
You are a helpful assistant. Use the context to answer the question.

Context:
{context}

Question: {question}

Answer:
"""
    if USE_OPENAI:
        openai.api_key = os.getenv("OPENAI_API_KEY")
        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    else:
        import requests
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "llama3", "prompt": prompt, "stream": False}
        )
        return response.json()["response"].strip()


# MAIN LOGIC
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["vector", "rerank", "gpt"], default="vector", help="Query mode")
    args = parser.parse_args()

    print(f"\nðŸ”Ž Mode selected: {args.mode}")
    while True:
        user_query = input("\nEnter your question: ").strip()
        if user_query.lower() in ["exit", "quit"]:
            break

        query_embedding = embedder.encode([user_query], normalize_embeddings=True)
        results = collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=TOP_K,
            include=["documents", "metadatas"]
        )

        raw_chunks = list(zip(results["documents"][0], results["metadatas"][0]))
        chunks = [(doc, meta) for doc, meta in raw_chunks if is_text_natural(doc)]
        docs_only = [doc for doc, _ in chunks]

        if args.mode == "vector":
            print("\nðŸ“Œ Top Vector Match:")
            print(docs_only[0])

        elif args.mode == "rerank":
            rerank_inputs = [[user_query, doc[:512]] for doc in docs_only]
            scores = reranker.predict(rerank_inputs)
            top_doc = docs_only[scores.argmax()]
            print("\nðŸ“Œ Best Reranked Chunk:")
            print(top_doc)

        elif args.mode == "gpt":
            rerank_inputs = [[user_query, doc[:512]] for doc in docs_only]
            scores = reranker.predict(rerank_inputs)
            top_chunks = [doc for _, doc in sorted(zip(scores, docs_only), key=lambda x: x[0], reverse=True)[:TOP_RERANKED]]
            print("\nðŸ¤– Answer (Generated by LLM):")
            print(generate_answer(top_chunks, user_query))

if __name__ == "__main__":
    main()
